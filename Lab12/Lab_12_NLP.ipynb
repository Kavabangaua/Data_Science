{"cells":[{"cell_type":"markdown","id":"5c4b07ba","metadata":{},"source":["\n","# Лабораторна робота 12\n","## Основи обробки природної мови (NLP)\n","**Мета:** Познайомитися з базовими техніками NLP, такими як токенізація, лемматизація, векторизація тексту, а також навчитися застосовувати прості моделі для класифікації текстів.\n"]},{"cell_type":"code","execution_count":1,"id":"a695e8d5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/alexeipavlenko/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/alexeipavlenko/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/alexeipavlenko/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","import string\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n"]},{"cell_type":"code","execution_count":2,"id":"c0dc7dac","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From: menon@boulder.Colorado.EDU (Ravi or Dean...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From: pjc@jet.uk (Peter J Card)\\nSubject: Re: ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From: arthurc@sfsuvax1.sfsu.edu (Arthur Chandl...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>From: mrf4276@egbsun12.NoSubdomain.NoDomain (M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From: sysmgr@king.eng.umd.edu (Doug Mohney)\\nS...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  target\n","0  From: menon@boulder.Colorado.EDU (Ravi or Dean...       0\n","1  From: pjc@jet.uk (Peter J Card)\\nSubject: Re: ...       1\n","2  From: arthurc@sfsuvax1.sfsu.edu (Arthur Chandl...       1\n","3  From: mrf4276@egbsun12.NoSubdomain.NoDomain (M...       1\n","4  From: sysmgr@king.eng.umd.edu (Doug Mohney)\\nS...       1"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","categories = ['sci.med', 'sci.space']\n","dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n","data = pd.DataFrame({'text': dataset.data, 'target': dataset.target})\n","\n","data.head()\n"]},{"cell_type":"code","execution_count":5,"id":"0f175909","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  \\\n","0  From: menon@boulder.Colorado.EDU (Ravi or Dean...   \n","1  From: pjc@jet.uk (Peter J Card)\\nSubject: Re: ...   \n","2  From: arthurc@sfsuvax1.sfsu.edu (Arthur Chandl...   \n","3  From: mrf4276@egbsun12.NoSubdomain.NoDomain (M...   \n","4  From: sysmgr@king.eng.umd.edu (Doug Mohney)\\nS...   \n","\n","                                          clean_text  \n","0  from: menon@boulder.colorado.edu (ravi or dean...  \n","1  from: pjc@jet.uk (peter j card)\\nsubject: re: ...  \n","2  from: arthurc@sfsuvax1.sfsu.edu (arthur chandl...  \n","3  from: mrf4276@egbsun12.nosubdomain.nodomain (m...  \n","4  from: sysmgr@king.eng.umd.edu (doug mohney)\\ns...  \n"]}],"source":["stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text(text):\n","    try:\n","        text = text.lower()\n","        tokens = word_tokenize(text) \n","        tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n","        tokens = [lemmatizer.lemmatize(word) for word in tokens] \n","        return ' '.join(tokens)\n","    except LookupError:\n","        return text \n","\n","data['clean_text'] = data['text'].apply(preprocess_text)\n","\n","if 'clean_text' in data.columns:\n","    print(data[['text', 'clean_text']].head())\n","else:\n","    print(\"Error: Column 'clean_text' was not created.\")\n"]},{"cell_type":"code","execution_count":6,"id":"251364ff","metadata":{},"outputs":[{"data":{"text/plain":["(1977, 32977)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(data['clean_text'])\n","y = data['target']\n","\n","X.shape\n"]},{"cell_type":"code","execution_count":7,"id":"38adefc3","metadata":{},"outputs":[],"source":["\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":8,"id":"086ddec5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Naive Bayes Accuracy: 0.9772727272727273\n","Confusion Matrix:\n","[[189   4]\n"," [  5 198]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.98      0.98       193\n","           1       0.98      0.98      0.98       203\n","\n","    accuracy                           0.98       396\n","   macro avg       0.98      0.98      0.98       396\n","weighted avg       0.98      0.98      0.98       396\n","\n"]}],"source":["nb_model = MultinomialNB()\n","nb_model.fit(X_train, y_train)\n","nb_pred = nb_model.predict(X_test)\n","\n","nb_accuracy = accuracy_score(y_test, nb_pred)\n","print(f\"Naive Bayes Accuracy: {nb_accuracy}\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, nb_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, nb_pred))\n"]},{"cell_type":"code","execution_count":9,"id":"8ab30524","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression Accuracy: 0.9646464646464646\n","Confusion Matrix:\n","[[191   2]\n"," [ 12 191]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.99      0.96       193\n","           1       0.99      0.94      0.96       203\n","\n","    accuracy                           0.96       396\n","   macro avg       0.97      0.97      0.96       396\n","weighted avg       0.97      0.96      0.96       396\n","\n"]}],"source":["lr_model = LogisticRegression(max_iter=1000)\n","lr_model.fit(X_train, y_train)\n","lr_pred = lr_model.predict(X_test)\n","\n","lr_accuracy = accuracy_score(y_test, lr_pred)\n","print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, lr_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, lr_pred))\n"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}
